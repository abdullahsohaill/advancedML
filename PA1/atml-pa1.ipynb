{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Assignment 1, Task 1: CNN vs. ViT Inductive Biases\n**By:** Muhammad Abdullah Sohail, Muhammad Haseeb, and Salaar Masood.\n\n**Models:** ResNet-50 vs. ViT-S/16  \n**Dataset:** CIFAR-10\n\nThis notebook contains the complete code for investigating and comparing the inductive biases of a Convolutional Neural Network (ResNet-50) and a Vision Transformer (ViT-S/16). The experiments are structured to test semantic, architectural, and generalization biases as outlined in the assignment brief.","metadata":{}},{"cell_type":"markdown","source":"### Setup and Imports","metadata":{}},{"cell_type":"code","source":"# Install required libraries, especially 'timm' for the ViT-S/16 model\n!pip install -q timm scikit-learn seaborn umap-learn\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import datasets, models, transforms\nfrom torch.utils.data import DataLoader, Dataset\nimport timm # PyTorch Image Models library\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport time\nimport copy\nimport os\nfrom PIL import Image\nfrom tqdm.notebook import tqdm\n\nfrom sklearn.manifold import TSNE\nimport umap\n\n# --- Configuration ---\n# Set seed for reproducibility\ntorch.manual_seed(42)\nnp.random.seed(42)\n\n# Set up device (GPU is highly recommended)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Matplotlib style\nplt.style.use('seaborn-v0_8-whitegrid')\n%matplotlib inline","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Helper Functions (Training & Evaluation)","metadata":{}},{"cell_type":"code","source":"# This cell contains reusable helper functions for training and evaluating models.\n\ndef train_model(model, train_loader, val_loader, num_epochs=10, lr=0.001, model_save_path='model.pth'):\n    \"\"\"Trains a model and saves the best performing one.\"\"\"\n    criterion = nn.CrossEntropyLoss()\n    # Observe that only parameters of the final layer are being optimized.\n    optimizer = optim.SGD(filter(lambda p: p.requires_grad, model.parameters()), lr=lr, momentum=0.9)\n    \n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_acc = 0.0\n    \n    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n\n    for epoch in range(num_epochs):\n        print(f'Epoch {epoch+1}/{num_epochs}')\n        print('-' * 10)\n\n        for phase in ['train', 'val']:\n            if phase == 'train':\n                model.train()\n                dataloader = train_loader\n            else:\n                model.eval()\n                dataloader = val_loader\n\n            running_loss = 0.0\n            running_corrects = 0\n\n            for inputs, labels in tqdm(dataloader, desc=f\"{phase.capitalize()} Epoch {epoch+1}\"):\n                inputs, labels = inputs.to(device), labels.to(device)\n                optimizer.zero_grad()\n\n                with torch.set_grad_enabled(phase == 'train'):\n                    outputs = model(inputs)\n                    _, preds = torch.max(outputs, 1)\n                    loss = criterion(outputs, labels)\n\n                    if phase == 'train':\n                        loss.backward()\n                        optimizer.step()\n\n                running_loss += loss.item() * inputs.size(0)\n                running_corrects += torch.sum(preds == labels.data)\n\n            epoch_loss = running_loss / len(dataloader.dataset)\n            epoch_acc = running_corrects.double() / len(dataloader.dataset)\n            \n            history[f'{phase}_loss'].append(epoch_loss)\n            history[f'{phase}_acc'].append(epoch_acc.item())\n\n            print(f'{phase.capitalize()} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n\n            if phase == 'val' and epoch_acc > best_acc:\n                best_acc = epoch_acc\n                best_model_wts = copy.deepcopy(model.state_dict())\n                torch.save(model.state_dict(), model_save_path)\n                print(f\"New best model saved to {model_save_path} with accuracy: {best_acc:.4f}\")\n\n    print(f'Best val Acc: {best_acc:4f}')\n    model.load_state_dict(best_model_wts)\n    return model, history\n\ndef evaluate_model(model, dataloader, name=\"Test\"):\n    \"\"\"Evaluates a model's accuracy on a given dataloader.\"\"\"\n    model.eval()\n    running_corrects = 0\n    \n    with torch.no_grad():\n        for inputs, labels in tqdm(dataloader, desc=f\"Evaluating on {name}\"):\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            _, preds = torch.max(outputs, 1)\n            running_corrects += torch.sum(preds == labels.data)\n            \n    accuracy = running_corrects.double() / len(dataloader.dataset)\n    print(f'{name} Accuracy: {accuracy:.4f}')\n    return accuracy.item()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 1: Model Fine-tuning on CIFAR-10\n\nFirst, we fine-tune a pre-trained ResNet-50 and a ViT-S/16 on the CIFAR-10 training data. This establishes our baseline in-distribution performance. We replace the final classifier of each model to match the 10 classes of CIFAR-10 and train them until they reach a reasonable accuracy.","metadata":{}},{"cell_type":"code","source":"# --- Data Preparation ---\n# Standard ImageNet transforms, as models are pre-trained on it.\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\ntrain_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\ntest_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n\ntrain_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=2)\ntest_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=2)\n\n# --- Model Definition ---\ndef get_model(model_name, num_classes=10, pretrained=True):\n    if model_name == 'resnet50':\n        model = models.resnet50(weights=models.ResNet50_Weights.DEFAULT if pretrained else None)\n        # Unfreeze all parameters for fine-tuning\n        for param in model.parameters():\n            param.requires_grad = True\n        num_ftrs = model.fc.in_features\n        model.fc = nn.Linear(num_ftrs, num_classes)\n        \n    elif model_name == 'vit_s_16':\n        # Using timm to get the ViT-S/16 model\n        model = timm.create_model('vit_small_patch16_224', pretrained=pretrained, num_classes=num_classes)\n    else:\n        raise ValueError(\"Model not supported\")\n        \n    return model.to(device)\n\n# Instantiate the models\nresnet50 = get_model('resnet50')\nvit_s16 = get_model('vit_s_16')\n\nprint(\"ResNet-50 and ViT-S/16 models created.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# NOTE: Training will take a significant amount of time. \n# Run this once and the best models will be saved.\n# If you have already trained models, you can skip this cell and load them in the next one.\n\nprint(\"--- Training ResNet-50 ---\")\nresnet50_trained, resnet_history = train_model(resnet50, train_loader, test_loader, num_epochs=10, model_save_path='resnet50_cifar10.pth')\n\nprint(\"\\n--- Training ViT-S/16 ---\")\nvit_s16_trained, vit_history = train_model(vit_s16, train_loader, test_loader, num_epochs=10, model_save_path='vit_s16_cifar10.pth')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 2 & 3: In-Distribution Performance and Color Bias Test\n\nNow, we load our fine-tuned models and evaluate their baseline accuracy on the clean CIFAR-10 test set. Then, we test them on a grayscale version of the test set to measure their reliance on color cues. A significant drop in accuracy indicates a strong color bias.","metadata":{}},{"cell_type":"code","source":"# --- Load Pre-trained Models ---\n# If you skipped the training cell, run this to load your saved models.\nresnet50 = get_model('resnet50')\nresnet50.load_state_dict(torch.load('resnet50_cifar10.pth'))\n\nvit_s16 = get_model('vit_s_16')\nvit_s16.load_state_dict(torch.load('vit_s16_cifar10.pth'))\n\n# --- Create Grayscale Test Set ---\ngrayscale_transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.Grayscale(num_output_channels=3), # Crucial for model input shape\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\ngrayscale_test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=grayscale_transform)\ngrayscale_loader = DataLoader(grayscale_test_dataset, batch_size=64, shuffle=False, num_workers=2)\n\n# --- Evaluate ---\nresults = {}\n\nprint(\"--- Evaluating ResNet-50 ---\")\nresults['resnet_clean'] = evaluate_model(resnet50, test_loader, \"Clean CIFAR-10\")\nresults['resnet_gray'] = evaluate_model(resnet50, grayscale_loader, \"Grayscale CIFAR-10\")\n\nprint(\"\\n--- Evaluating ViT-S/16 ---\")\nresults['vit_clean'] = evaluate_model(vit_s16, test_loader, \"Clean CIFAR-10\")\nresults['vit_gray'] = evaluate_model(vit_s16, grayscale_loader, \"Grayscale CIFAR-10\")\n\n# --- Analyze Color Bias ---\nresnet_drop = results['resnet_clean'] - results['resnet_gray']\nvit_drop = results['vit_clean'] - results['vit_gray']\n\nprint(f\"\\nAccuracy Drop (Color Bias):\")\nprint(f\"ResNet-50: {resnet_drop*100:.2f}%\")\nprint(f\"ViT-S/16: {vit_drop*100:.2f}%\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 5: Translation Invariance Test\n\nHere, we test the models' robustness to small spatial shifts in the input image. We create a test set where all images are shifted horizontally by a fixed number of pixels. CNNs, due to their convolutional nature (weight sharing), are expected to be more invariant to such shifts than ViTs.","metadata":{}},{"cell_type":"code","source":"# --- Create Translated Test Set ---\nshift_pixels = 16 # A noticeable shift\nimage_size = 224\ntranslate_fraction = shift_pixels / image_size\n\ntranslate_transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    # Apply a fixed horizontal shift\n    transforms.RandomAffine(degrees=0, translate=(translate_fraction, 0)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\ntranslated_test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=translate_transform)\ntranslated_loader = DataLoader(translated_test_dataset, batch_size=64, shuffle=False, num_workers=2)\n\n# --- Evaluate ---\nprint(\"--- Evaluating ResNet-50 on Translated Data ---\")\nresults['resnet_translated'] = evaluate_model(resnet50, translated_loader, f\"Translated ({shift_pixels}px)\")\n\nprint(\"\\n--- Evaluating ViT-S/16 on Translated Data ---\")\nresults['vit_translated'] = evaluate_model(vit_s16, translated_loader, f\"Translated ({shift_pixels}px)\")\n\n# --- Analyze Translation Invariance ---\nresnet_drop_translate = results['resnet_clean'] - results['resnet_translated']\nvit_drop_translate = results['vit_clean'] - results['vit_translated']\n\nprint(f\"\\nAccuracy Drop (Translation):\")\nprint(f\"ResNet-50: {resnet_drop_translate*100:.2f}%\")\nprint(f\"ViT-S/16: {vit_drop_translate*100:.2f}%\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 6: Permutation / Occlusion Test\n\nThis section tests how models react when the global structure of an image is disrupted.\n1.  **Patch Shuffling:** We break the image into a grid of patches and shuffle their positions.\n2.  **Occlusion:** We randomly mask out a square region of the image.\n\nThis helps reveal whether the models rely more on local features (which are preserved in shuffling) or global context (which is destroyed).","metadata":{}},{"cell_type":"code","source":"# --- Custom Transform for Patch Shuffling ---\nclass PatchShuffler:\n    def __init__(self, patch_size=16):\n        self.patch_size = patch_size\n\n    def __call__(self, img_tensor):\n        c, h, w = img_tensor.shape\n        num_patches = (h // self.patch_size) * (w // self.patch_size)\n        \n        # Create a view of the image as patches\n        patches = img_tensor.unfold(1, self.patch_size, self.patch_size).unfold(2, self.patch_size, self.patch_size)\n        patches = patches.contiguous().view(c, -1, self.patch_size, self.patch_size)  # [C, Num_Patches, PS, PS]\n        \n        # Shuffle patches\n        perm = torch.randperm(patches.size(1))\n        shuffled_patches = patches[:, perm, :, :]\n        \n        # Reassemble the image\n        shuffled_patches = shuffled_patches.view(c, h // self.patch_size, w // self.patch_size, self.patch_size, self.patch_size)\n        shuffled_img = shuffled_patches.permute(0, 1, 3, 2, 4).contiguous().view(c, h, w)\n        return shuffled_img\n\n# --- Create Perturbed Datasets ---\npatch_shuffle_transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    PatchShuffler(patch_size=16), # Custom transform\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\nocclusion_transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    # Built-in transform for occlusion\n    transforms.RandomErasing(p=1.0, scale=(0.1, 0.2), ratio=(0.5, 2.0)), \n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\nshuffled_test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=patch_shuffle_transform)\noccluded_test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=occlusion_transform)\n\nshuffled_loader = DataLoader(shuffled_test_dataset, batch_size=64, shuffle=False)\noccluded_loader = DataLoader(occluded_test_dataset, batch_size=64, shuffle=False)\n\n# --- Evaluate ---\nprint(\"--- Evaluating on Patch-Shuffled Data ---\")\nresults['resnet_shuffled'] = evaluate_model(resnet50, shuffled_loader, \"Patch Shuffled\")\nresults['vit_shuffled'] = evaluate_model(vit_s16, shuffled_loader, \"Patch Shuffled\")\n\nprint(\"\\n--- Evaluating on Occluded Data ---\")\nresults['resnet_occluded'] = evaluate_model(resnet50, occluded_loader, \"Occluded\")\nresults['vit_occluded'] = evaluate_model(vit_s16, occluded_loader, \"Occluded\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 7: Feature Representation Analysis\n\nTo understand *how* the models represent data internally, we extract the penultimate layer features for a subset of test images. We then use UMAP to visualize the 2D feature space, coloring points by their class. This can reveal whether one model creates more semantically meaningful and separable clusters.","metadata":{}},{"cell_type":"code","source":"def get_features(model, dataloader, num_samples=500):\n    \"\"\"Extracts penultimate features from a model.\"\"\"\n    model.eval()\n    \n    # Create a feature extractor model\n    if isinstance(model, models.ResNet):\n        feature_extractor = nn.Sequential(*list(model.children())[:-1]) # Remove final fc layer\n    elif isinstance(model, timm.models.VisionTransformer):\n        feature_extractor = model.forward_features # Use timm's built-in method\n    else:\n        raise TypeError(\"Unsupported model type for feature extraction\")\n\n    features_list = []\n    labels_list = []\n    \n    with torch.no_grad():\n        for inputs, labels in tqdm(dataloader, desc=\"Extracting Features\", total=int(np.ceil(num_samples / dataloader.batch_size))):\n            inputs = inputs.to(device)\n            \n            # Get features\n            feats = feature_extractor(inputs)\n            # Flatten features: for ResNet it's (B, C, 1, 1), for ViT it's (B, N, D)\n            if feats.dim() == 4:\n                feats = feats.squeeze(-1).squeeze(-1)\n            elif 'vit' in model.default_cfg['architecture']:\n                feats = feats[:, 0] # Take the CLS token embedding\n\n            features_list.append(feats.cpu())\n            labels_list.append(labels)\n            \n            if len(torch.cat(labels_list)) >= num_samples:\n                break\n\n    return torch.cat(features_list).numpy(), torch.cat(labels_list).numpy()\n\n# --- Extract Features ---\nresnet_features, resnet_labels = get_features(resnet50, test_loader)\nvit_features, vit_labels = get_features(vit_s16, test_loader)\n\n# --- Visualize with UMAP ---\nprint(\"Running UMAP on ResNet-50 features...\")\nreducer = umap.UMAP(n_components=2, random_state=42)\nresnet_2d = reducer.fit_transform(resnet_features)\n\nprint(\"Running UMAP on ViT-S/16 features...\")\nvit_2d = reducer.fit_transform(vit_features)\n\n# Plotting\nfig, axes = plt.subplots(1, 2, figsize=(16, 7))\nclass_names = test_dataset.classes\n\n# ResNet Plot\nsns.scatterplot(x=resnet_2d[:, 0], y=resnet_2d[:, 1], hue=[class_names[l] for l in resnet_labels], \n                palette='tab10', s=10, alpha=0.7, ax=axes[0]).set_title('ResNet-50 Feature Space (UMAP)')\naxes[0].legend(title='Class', markerscale=1, fontsize='small')\n\n# ViT Plot\nsns.scatterplot(x=vit_2d[:, 0], y=vit_2d[:, 1], hue=[class_names[l] for l in vit_labels], \n                palette='tab10', s=10, alpha=0.7, ax=axes[1]).set_title('ViT-S/16 Feature Space (UMAP)')\naxes[1].legend(title='Class', markerscale=1, fontsize='small')\n\nplt.tight_layout()\nplt.savefig(\"feature_space_comparison.png\")\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Task 1 Summary of Quantitative Results\n\nFinally, we compile all our accuracy measurements into a single table to provide a clear, quantitative comparison of the two models across all tests.","metadata":{}},{"cell_type":"code","source":"# Create a pandas DataFrame for a clean summary\nsummary_df = pd.DataFrame({\n    'Test': ['Clean', 'Grayscale', 'Translated', 'Shuffled', 'Occluded'],\n    'ResNet-50 Acc': [\n        results.get('resnet_clean', 0),\n        results.get('resnet_gray', 0),\n        results.get('resnet_translated', 0),\n        results.get('resnet_shuffled', 0),\n        results.get('resnet_occluded', 0)\n    ],\n    'ViT-S/16 Acc': [\n        results.get('vit_clean', 0),\n        results.get('vit_gray', 0),\n        results.get('vit_translated', 0),\n        results.get('vit_shuffled', 0),\n        results.get('vit_occluded', 0)\n    ]\n})\n\n# Calculate accuracy drop from clean baseline\nsummary_df['ResNet-50 Drop'] = summary_df['ResNet-50 Acc'][0] - summary_df['ResNet-50 Acc']\nsummary_df['ViT-S/16 Drop'] = summary_df['ViT-S/16 Acc'][0] - summary_df['ViT-S/16 Acc']\n\n# Format as percentage\nfor col in summary_df.columns[1:]:\n    summary_df[col] = summary_df[col].apply(lambda x: f\"{x*100:.2f}%\")\n\nprint(\"--- Summary of Model Performance on Perturbation Tests ---\")\ndisplay(summary_df)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}