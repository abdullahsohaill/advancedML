{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d90dc20b",
   "metadata": {},
   "source": [
    "# Domain Generalization (ERM) on PACS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc9a1857",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x18ee2301030>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, ConcatDataset, Subset\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    " \n",
    "DATA_ROOT = Path(r\"C:\\Users\\Fatim_Sproj\\Desktop\\Fatim\\Spring 2025\\Datasets\\pacs_data\\pacs_data\") \n",
    "SOURCE_DOMAINS = [\"art_painting\", \"cartoon\", \"photo\"]\n",
    "TARGET_DOMAIN = \"sketch\"\n",
    "\n",
    "IMAGE_SIZE = 224\n",
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 10\n",
    "\n",
    "LR = 1e-4\n",
    "SEED = 42\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "VIT_MODEL_NAME = \"WinKawaks/vit-tiny-patch16-224\"\n",
    "\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5958167e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 4842, Val size: 1210, Target size: 3929\n",
      "Num classes: 7\n"
     ]
    }
   ],
   "source": [
    "imagenet_mean = [0.485, 0.456, 0.406]\n",
    "imagenet_std = [0.229, 0.224, 0.225]\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=imagenet_mean, std=imagenet_std),\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=imagenet_mean, std=imagenet_std),\n",
    "])\n",
    "\n",
    "\n",
    "def load_domain(domain_name, transform):\n",
    "    p = DATA_ROOT / domain_name\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(f\"Domain path not found: {p.resolve()}\")\n",
    "    ds = datasets.ImageFolder(str(p), transform=transform)\n",
    "    return ds\n",
    "\n",
    "source_datasets = {d: load_domain(d, train_transform) for d in SOURCE_DOMAINS}\n",
    "target_dataset = load_domain(TARGET_DOMAIN, val_transform)\n",
    "\n",
    "class_lists = [tuple(ds.classes) for ds in source_datasets.values()] + [tuple(target_dataset.classes)]\n",
    "if len(set(class_lists)) != 1:\n",
    "    print(\"WARNING: Class lists differ between domains. Ensure class folders match and are ordered the same.\")\n",
    "\n",
    "NUM_CLASSES = len(next(iter(source_datasets.values())).classes)\n",
    "\n",
    "train_dataset = ConcatDataset(list(source_datasets.values()))\n",
    "val_fraction = 0.2\n",
    "num_val = int(len(train_dataset) * val_fraction)\n",
    "num_train = len(train_dataset) - num_val\n",
    "train_subset = Subset(train_dataset, list(range(0, num_train)))\n",
    "val_subset = Subset(train_dataset, list(range(num_train, num_train + num_val)))\n",
    "\n",
    "train_loader = DataLoader(train_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(val_subset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "target_loader = DataLoader(target_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "source_loaders = {d: DataLoader(ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)\n",
    "                  for d, ds in source_datasets.items()}\n",
    "\n",
    "print(f\"Train size: {len(train_subset)}, Val size: {len(val_subset)}, Target size: {len(target_dataset)}\")\n",
    "print(f\"Num classes: {NUM_CLASSES}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b2e56c36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Fatim_Sproj\\anaconda3\\envs\\bacp\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Fatim_Sproj\\anaconda3\\envs\\bacp\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "\n",
    "class DomainBedModel(nn.Module):\n",
    "    def __init__(self, featurizer, classifier, dropout_rate=0.0):\n",
    "        super().__init__()\n",
    "        self.featurizer = featurizer\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.fc = classifier\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.featurizer(x)\n",
    "        features_dropped_out = self.dropout(features)\n",
    "        return self.fc(features_dropped_out)\n",
    "\n",
    "resnet_dropout = 0.0\n",
    "freeze_bn_flag = True\n",
    "\n",
    "base_model = torchvision.models.resnet18(pretrained=True)\n",
    "n_outputs = base_model.fc.in_features\n",
    "base_model.fc = nn.Identity()\n",
    "featurizer = base_model\n",
    "\n",
    "if freeze_bn_flag:\n",
    "    for module in featurizer.modules():\n",
    "        if isinstance(module, nn.BatchNorm2d):\n",
    "            module.eval()\n",
    "\n",
    "classifier = nn.Linear(n_outputs, NUM_CLASSES)\n",
    "\n",
    "model = DomainBedModel(\n",
    "    featurizer,\n",
    "    classifier,\n",
    "    dropout_rate=resnet_dropout\n",
    ").to(DEVICE)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "acf18c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim import Optimizer\n",
    "\n",
    "class SAM(Optimizer):\n",
    "    def __init__(self, params, base_optimizer, rho=0.05, adaptive=False, **kwargs):\n",
    "        params = list(params)\n",
    "        params = [p for p in params if p is not None and p.requires_grad]\n",
    "        if len(params) == 0:\n",
    "            raise ValueError(\"SAM received no parameters to optimize. \"\n",
    "                             \"Did you pass model.parameters() after they've been consumed or did you freeze all params?\")\n",
    "\n",
    "        defaults = dict(rho=rho, adaptive=adaptive, **kwargs)\n",
    "        super().__init__(params, defaults)\n",
    "        self.base_optimizer = base_optimizer(self.param_groups, **kwargs)\n",
    "        self.adaptive = adaptive\n",
    "        self.rho = rho\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def first_step(self, zero_grad=True):\n",
    "        grad_norm = self._grad_norm()\n",
    "        scale = self.rho / (grad_norm + 1e-12)\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                if self.adaptive:\n",
    "                    e_w = (torch.abs(p) * p.grad) * scale\n",
    "                else:\n",
    "                    e_w = p.grad * scale\n",
    "                p.add_(e_w)                 \n",
    "                self.state[p]['e_w'] = e_w  \n",
    "\n",
    "        if zero_grad:\n",
    "            self.zero_grad()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def second_step(self, zero_grad=True):\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                e_w = self.state[p].get('e_w', None)\n",
    "                if e_w is None:\n",
    "                    continue\n",
    "                p.sub_(e_w)  \n",
    "        self.base_optimizer.step()\n",
    "        if zero_grad:\n",
    "            self.zero_grad()\n",
    "\n",
    "    def zero_grad(self):\n",
    "        self.base_optimizer.zero_grad()\n",
    "\n",
    "    def _grad_norm(self):\n",
    "        device = self.param_groups[0]['params'][0].device\n",
    "        norms = []\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                if self.adaptive:\n",
    "                    norms.append(((torch.abs(p) * p.grad).norm(p=2)).to(device))\n",
    "                else:\n",
    "                    norms.append((p.grad.norm(p=2)).to(device))\n",
    "        if not norms:\n",
    "            return torch.tensor(0.0, device=device)\n",
    "        stacked = torch.stack(norms)\n",
    "        return torch.norm(stacked, p=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a7485595",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "def train_one_epoch_sam(model, loader, optimizer, device, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    pbar = tqdm(loader, desc=\"Train (SAM)\", leave=False)\n",
    "    for imgs, labels in pbar:\n",
    "        imgs = imgs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        def closure():\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(imgs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            return loss\n",
    "\n",
    "        closure_loss = closure()\n",
    "        optimizer.first_step(zero_grad=True)\n",
    "        closure_loss = closure()\n",
    "        optimizer.second_step(zero_grad=True)\n",
    "\n",
    "        outputs = model(imgs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        total_loss += loss.item() * imgs.size(0)\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += imgs.size(0)\n",
    "        pbar.set_postfix({'loss': total_loss/total, 'acc': 100*correct/total})\n",
    "\n",
    "    return total_loss/total, 100*correct/total\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    total_loss = 0.0\n",
    "    for imgs, labels in loader:\n",
    "        imgs = imgs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(imgs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        total_loss += loss.item() * imgs.size(0)\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += imgs.size(0)\n",
    "    return total_loss/total, 100*correct/total\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7e3cbcdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Train loss: 0.7529 acc: 79.35% | Target loss: 1.5186 acc: 44.03%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 | Train loss: 0.4569 acc: 88.93% | Target loss: 1.1649 acc: 66.51%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 | Train loss: 0.3225 acc: 93.10% | Target loss: 1.0704 acc: 71.77%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 | Train loss: 0.2361 acc: 95.17% | Target loss: 0.9734 acc: 69.25%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 | Train loss: 0.1955 acc: 96.03% | Target loss: 0.9127 acc: 68.44%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 | Train loss: 0.1530 acc: 97.01% | Target loss: 0.8684 acc: 73.15%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 | Train loss: 0.1247 acc: 97.77% | Target loss: 0.9399 acc: 72.92%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 | Train loss: 0.0993 acc: 98.31% | Target loss: 0.9209 acc: 70.45%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10 | Train loss: 0.0888 acc: 98.62% | Target loss: 0.9392 acc: 71.82%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 | Train loss: 0.0793 acc: 98.66% | Target loss: 0.8104 acc: 74.85%\n",
      "\n",
      "Best target-domain accuracy (SAM): 74.85% at epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Fatim_Sproj\\AppData\\Local\\Temp\\ipykernel_21980\\598782616.py:39: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(best_model_path, map_location=DEVICE))\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import AdamW\n",
    "rho = 0.5\n",
    "sam_optimizer = SAM(\n",
    "    [p for p in model.parameters() if p.requires_grad],\n",
    "    AdamW,\n",
    "    lr=LR,\n",
    "    weight_decay=1e-4,\n",
    "    rho=rho\n",
    ")\n",
    "\n",
    "best_target_acc = 0.0\n",
    "best_epoch = 0\n",
    "metrics = {\"epochs\": []}\n",
    "\n",
    "OUTPUT_DIR = Path(\"./sam_outputs\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    train_loss, train_acc = train_one_epoch_sam(model, train_loader, sam_optimizer, DEVICE, criterion)\n",
    "    target_loss, target_acc = evaluate(model, target_loader, DEVICE)\n",
    "    print(f\"Epoch {epoch}/{NUM_EPOCHS} | Train loss: {train_loss:.4f} acc: {train_acc:.2f}% | Target loss: {target_loss:.4f} acc: {target_acc:.2f}%\")\n",
    "\n",
    "    metrics[\"epochs\"].append({\n",
    "        \"epoch\": epoch,\n",
    "        \"train_loss\": train_loss,\n",
    "        \"train_acc\": train_acc,\n",
    "        \"target_loss\": target_loss,\n",
    "        \"target_acc\": target_acc\n",
    "    })\n",
    "\n",
    "    if target_acc > best_target_acc:\n",
    "        best_target_acc = target_acc\n",
    "        best_epoch = epoch\n",
    "        torch.save(model.state_dict(), OUTPUT_DIR / \"best_model.pth\")\n",
    "\n",
    "print(f\"\\nBest target-domain accuracy (SAM): {best_target_acc:.2f}% at epoch {best_epoch}\")\n",
    "\n",
    "best_model_path = OUTPUT_DIR / \"best_model.pth\"\n",
    "model.load_state_dict(torch.load(best_model_path, map_location=DEVICE))\n",
    "model.to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "domain_results = {}\n",
    "total_acc = 0.0\n",
    "for d, loader in source_loaders.items():\n",
    "    _, acc = evaluate(model, loader, DEVICE)\n",
    "    domain_results[d] = acc\n",
    "    total_acc += acc\n",
    "\n",
    "t_loss, t_acc = evaluate(model, target_loader, DEVICE)\n",
    "domain_results[TARGET_DOMAIN] = t_acc\n",
    "total_acc += t_acc\n",
    "mean_acc = total_acc / (len(source_loaders) + 1)\n",
    "mean_source_acc = total_acc - t_acc\n",
    "mean_source_acc /= len(source_loaders)\n",
    "\n",
    "metrics.update({\n",
    "    \"best_target_acc\": best_target_acc,\n",
    "    \"best_epoch\": best_epoch,\n",
    "    \"mean_acc\": mean_acc,\n",
    "    \"mean_source_acc\": mean_source_acc,\n",
    "    \"domain_results\": domain_results\n",
    "})\n",
    "\n",
    "with open(OUTPUT_DIR / \"metrics.json\", \"w\") as f:\n",
    "    json.dump(metrics, f, indent=2)\n",
    "\n",
    "epochs = [m[\"epoch\"] for m in metrics[\"epochs\"]]\n",
    "train_accs = [m[\"train_acc\"] for m in metrics[\"epochs\"]]\n",
    "target_accs = [m[\"target_acc\"] for m in metrics[\"epochs\"]]\n",
    "train_losses = [m[\"train_loss\"] for m in metrics[\"epochs\"]]\n",
    "target_losses = [m[\"target_loss\"] for m in metrics[\"epochs\"]]\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(epochs, train_accs, label=\"Train Acc\")\n",
    "plt.plot(epochs, target_accs, label=\"Target Acc\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy (%)\")\n",
    "plt.legend()\n",
    "plt.title(\"SAM Training and Target Accuracy\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / \"acc_curves.png\")\n",
    "plt.close()\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(epochs, train_losses, label=\"Train Loss\")\n",
    "plt.plot(epochs, target_losses, label=\"Target Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.title(\"SAM Training and Target Loss\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / \"loss_curves.png\")\n",
    "plt.close()\n",
    "\n",
    "domains = list(domain_results.keys())\n",
    "accuracies = [domain_results[d] for d in domains]\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(domains, accuracies)\n",
    "plt.ylabel(\"Accuracy (%)\")\n",
    "plt.title(\"Per-Domain Accuracy (SAM Final Model)\")\n",
    "plt.xticks(rotation=30, ha=\"right\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / \"domain_accuracy_bar.png\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2d8da994",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Fatim_Sproj\\AppData\\Local\\Temp\\ipykernel_21980\\1286560994.py:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(best_model_path, map_location=DEVICE)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source art_painting: loss 0.1415, acc 98.09%\n",
      "Source cartoon: loss 0.0337, acc 99.45%\n",
      "Source photo: loss 0.2736, acc 92.81%\n",
      "\n",
      "Mean source accuracy: 96.78%\n",
      "Mean domain accuracy (incl. target): 91.30%\n",
      "Saved final model to erm_outputs/\n"
     ]
    }
   ],
   "source": [
    "best_model_path = OUTPUT_DIR / 'best_model.pth'\n",
    "\n",
    "model = DomainBedModel(\n",
    "    featurizer,\n",
    "    classifier,\n",
    "    dropout_rate=resnet_dropout\n",
    ").to(DEVICE)\n",
    "\n",
    "state_dict = torch.load(best_model_path, map_location=DEVICE)\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval()\n",
    "\n",
    "domain_results = {}\n",
    "total_acc = 0.0\n",
    "\n",
    "for d, loader in source_loaders.items():\n",
    "    loss, acc = evaluate(model, loader, DEVICE)\n",
    "    domain_results[d] = {\"loss\": loss, \"acc\": acc}\n",
    "    total_acc += acc\n",
    "    print(f\"Source {d}: loss {loss:.4f}, acc {acc:.2f}%\")\n",
    "\n",
    "t_loss, t_acc = evaluate(model, target_loader, DEVICE)\n",
    "domain_results[TARGET_DOMAIN] = {\"loss\": t_loss, \"acc\": t_acc}\n",
    "total_acc += t_acc\n",
    "\n",
    "mean_source_acc = total_acc - t_acc\n",
    "mean_source_acc /= len(source_loaders)\n",
    "mean_acc = total_acc / (len(source_loaders) + 1)\n",
    "\n",
    "print(f\"\\nMean source accuracy: {mean_source_acc:.2f}%\")\n",
    "print(f\"Mean domain accuracy (incl. target): {mean_acc:.2f}%\")\n",
    "\n",
    "final_results = {\n",
    "    \"best_target_acc\": best_target_acc,\n",
    "    \"mean_source_acc\": mean_source_acc,\n",
    "    \"domain_results\": domain_results,\n",
    "    \"mean_acc\": mean_acc\n",
    "}\n",
    "\n",
    "torch.save(model, OUTPUT_DIR / 'final_model_full.pth')\n",
    "print(\"Saved final model to erm_outputs/\")\n",
    "\n",
    "domains = list(domain_results.keys())\n",
    "accuracies = [domain_results[d][\"acc\"] for d in domains]\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "bars = plt.bar(domains, accuracies, color='skyblue', edgecolor='black', linewidth=1.2)\n",
    "plt.ylabel(\"Accuracy (%)\", fontsize=12)\n",
    "plt.title(\"Per-Domain Accuracy (ERM Final Model)\", fontsize=13, pad=10)\n",
    "\n",
    "plt.xticks(range(len(domains)), domains, rotation=0, ha='center', fontsize=11)\n",
    "plt.yticks(fontsize=11)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.6)\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / \"domain_accuracy_bar.png\", dpi=300, bbox_inches='tight')\n",
    "plt.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bacp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
